{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiomungo/ecampus/blob/main/02_tf_matrix_multiplication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc27e055",
      "metadata": {
        "id": "bc27e055"
      },
      "source": [
        "# Moltiplicazione di matrici con TensorFlow (tf.matmul)\n",
        "\n",
        "> - Istruttore: prof. Fabio Mungo - eCampus Master AI Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79868955",
      "metadata": {
        "id": "79868955"
      },
      "source": [
        "## MatMul base: (m×n) @ (n×p) -> (m×p)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "try...except Exception as e:: Questo è un blocco di gestione delle eccezioni. Significa che il codice all'interno del blocco try verrà eseguito e, se si verifica un errore (un'eccezione), l'esecuzione passerà al blocco except. Questo è utile per gestire situazioni come TensorFlow che non è installato o configurato correttamente.\n",
        "\n",
        "import tensorflow as tf: Questa riga importa la libreria TensorFlow, che è un framework open-source ampiamente utilizzato per il machine learning e il deep learning. Viene importata con l'alias tf per rendere il suo utilizzo più conciso nel codice.\n",
        "\n",
        "A = tf.random.normal((2,3)): Qui viene creato un tensore (che è l'equivalente di una matrice o un array multidimensionale in TensorFlow) chiamato A.:\n",
        "\n",
        "tf.random.normal(): Questa funzione genera numeri casuali da una distribuzione normale (gaussiana) con media 0 e deviazione standard 1.\n",
        "(2,3): Questo specifica la forma del tensore, il che significa che A sarà una matrice con 2 righe e 3 colonne.\n",
        "B = tf.random.normal((3,4)): Allo stesso modo, viene creato un secondo tensore chiamato B con numeri casuali da una distribuzione normale. B avrà una forma di (3,4), ovvero 3 righe e 4 colonne.\n",
        "\n",
        "C = tf.matmul(A,B): Questa è l'operazione centrale: la moltiplicazione di matrici. tf.matmul() esegue il prodotto matriciale standard tra A e B.\n",
        "\n",
        "Compatibilità: Per eseguire la moltiplicazione matriciale A @ B, il numero di colonne della matrice A deve essere uguale al numero di righe della matrice B.\n",
        "Nel nostro caso, A ha forma (2, 3) e B ha forma (3, 4). Poiché 3 == 3, le matrici sono compatibili per la moltiplicazione.\n",
        "Forma del risultato: La matrice risultante C avrà un numero di righe uguale a quello di A e un numero di colonne uguale a quello di B.\n",
        "Quindi, C avrà forma (2, 4).\n",
        "print('A', A.shape, 'B', B.shape, 'C', C.shape): Questa riga stampa le forme (dimensioni) dei tre tensori: A, B e C. Questo è utile per verificare che la moltiplicazione matriciale sia avvenuta correttamente e che il tensore risultante C abbia la forma prevista (2,4).\n",
        "\n",
        "print('TF non disponibile:', e): Se TensorFlow non è installato o se si verifica un altro errore durante l'esecuzione del codice nel blocco try, questo messaggio verrà stampato, mostrando la causa dell'errore."
      ],
      "metadata": {
        "id": "5ZJVVI92R0DZ"
      },
      "id": "5ZJVVI92R0DZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20924cfe",
      "metadata": {
        "id": "20924cfe"
      },
      "outputs": [],
      "source": [
        "try:\n",
        " import tensorflow as tf\n",
        " A=tf.random.uniform((2,3), minval=0, maxval=10, dtype=tf.int32)\n",
        " B=tf.random.uniform((3,4), minval=0, maxval=10, dtype=tf.int32)\n",
        " C=tf.matmul(A,B)\n",
        " print('A=',A,\"\\n\",'B=',B,\"\\n\"'C=',C)\n",
        " print('A',A.shape,'B',B.shape,'C',C.shape)\n",
        "except Exception as e:\n",
        " print('TF non disponibile:', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "181d0c1d",
      "metadata": {
        "id": "181d0c1d"
      },
      "source": [
        "## Batch matmul: (batch,m,n) @ (batch,n,p)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yHT_TsBjSYW1"
      },
      "id": "yHT_TsBjSYW1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "questo codice dimostra la moltiplicazione di matrici batch (batch matmul) utilizzando TensorFlow. È simile all'esempio precedente, ma opera su più matrici contemporaneamente."
      ],
      "metadata": {
        "id": "78lALHyaSUY-"
      },
      "id": "78lALHyaSUY-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a73ef8bb",
      "metadata": {
        "id": "a73ef8bb"
      },
      "outputs": [],
      "source": [
        "try:\n",
        " import tensorflow as tf\n",
        " A=tf.random.normal((5,2,3))\n",
        " B=tf.random.normal((5,3,4))\n",
        " C=tf.matmul(A,B)\n",
        " print(C.shape)\n",
        "except Exception as e:\n",
        " print('TF non disponibile:', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e8af707",
      "metadata": {
        "id": "9e8af707"
      },
      "source": [
        "## Esercizio: matmul manuale in numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "questo codice Python implementa manualmente la moltiplicazione di matrici usando NumPy e poi ne verifica la correttezza confrontandola con l'operatore di moltiplicazione matriciale integrato di NumPy.\n",
        "\n",
        "**matmul_manual**: Dettaglia la logica interna della funzione matmul_manual, inclusa l'inizializzazione, il controllo delle dimensioni e i cicli annidati per il calcolo del prodotto.\n",
        "\n",
        "**Creazione di matrici di test**: Spiega come vengono create le matrici A e B di esempio per testare la funzione.\n",
        "\n",
        "**Confronto e verifica**: Descrivi l'ultima riga di codice che confronta il risultato della funzione manuale con il prodotto matriciale di NumPy e stampa la massima differenza.\n",
        "\n",
        "**Contesto e utilità**: Fornisci il contesto sull'utilità di questa implementazione manuale per comprendere i principi del deep learning."
      ],
      "metadata": {
        "id": "O5SDINEpS0VR"
      },
      "id": "O5SDINEpS0VR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7de239fb",
      "metadata": {
        "id": "7de239fb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def matmul_manual(A,B):\n",
        " A=np.asarray(A); B=np.asarray(B)\n",
        " m,n=A.shape\n",
        " n2,p=B.shape\n",
        " assert n==n2\n",
        " C=np.zeros((m,p))\n",
        " for i in range(m):\n",
        "  for j in range(p):\n",
        "   C[i,j]=(A[i,:]*B[:,j]).sum()\n",
        " return C\n",
        "\n",
        "A=np.random.randn(2,3)\n",
        "B=np.random.randn(3,4)\n",
        "print(np.max(np.abs(matmul_manual(A,B)-(A@B))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e91bcca"
      },
      "source": [
        "# Task\n",
        "add_text_cell(content='## Introduzione alle Operazioni Fondamentali nel Deep Learning')"
      ],
      "id": "9e91bcca"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cecb9060"
      },
      "source": [
        "## Introduzione Operazioni Deep Learning\n",
        "\n",
        "### Subtask:\n",
        "Aggiungi una cella di testo per introdurre la nuova sezione dedicata alle operazioni fondamentali nel Deep Learning.\n"
      ],
      "id": "cecb9060"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee8db9d9"
      },
      "source": [
        "## Introduzione alle Operazioni Fondamentali nel Deep Learning"
      ],
      "id": "ee8db9d9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62e39441"
      },
      "source": [
        "## Operazioni Matrice PyTorch\n",
        "\n",
        "### Subtask:\n",
        "Aggiungi una cella di codice che dimostra operazioni di base sulle matrici in PyTorch, inclusa la moltiplicazione (`torch.matmul`) e operazioni elemento per elemento, anche con broadcasting.\n"
      ],
      "id": "62e39441"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "905401bf"
      },
      "source": [
        "il codice nella cella sottostante dimostra diverse operazioni fondamentali sui tensori in PyTorch, inclusa la moltiplicazione di matrici, operazioni elemento per elemento e il broadcasting. Ecco una spiegazione dettagliata:\n",
        "\n",
        "1) Importazione della libreria PyTorch: Spiegazione della riga che importa la libreria torch.\n",
        "\n",
        "2) Creazione di tensori di esempio per la moltiplicazione di matrici: Dettagli sulla creazione di tensor_a e tensor_b con torch.randn e le loro forme.\n",
        "\n",
        "3) Esecuzione della moltiplicazione di matrici (torch.matmul): Spiegazione di come torch.matmul esegue il prodotto matriciale, inclusa la compatibilità delle dimensioni e la forma del risultato.\n",
        "\n",
        "4) Creazione di tensori di esempio per operazioni elemento per elemento: Dettagli sulla creazione di tensor_c e tensor_d con torch.randn.\n",
        "\n",
        "5) Esecuzione della moltiplicazione elemento per elemento: Spiegazione dell'operazione * per la moltiplicazione elemento per elemento tra tensori.\n",
        "\n",
        "6) Dimostrazione del Broadcasting con uno scalare: Spiegazione di come un tensore viene moltiplicato per uno scalare, estendendo implicitamente lo scalare a tutte le dimensioni del tensore.\n",
        "\n",
        "7) Dimostrazione del Broadcasting con un tensore: Spiegazione di un esempio più complesso di broadcasting, dove un tensore di dimensione (1,3) viene esteso per essere compatibile con un tensore (2,3) durante la moltiplicazione.\n",
        "\n",
        "8) Output delle forme dei tensori: Spiegazione dell'uso di print e .shape per visualizzare i tensori e le loro dimensioni in ogni passaggio.\n",
        "\n",
        "\n",
        "**Reasoning**:\n",
        "To fulfill the subtask, I need to add a code cell that demonstrates basic PyTorch matrix operations, including matrix multiplication, element-wise operations, and broadcasting, as specified in the instructions.\n",
        "\n"
      ],
      "id": "905401bf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce6bbfa4"
      },
      "source": [
        "import torch\n",
        "\n",
        "# 2. Crea due tensori di esempio\n",
        "tensor_a = torch.randn(2, 3) # Dimensione (2, 3)\n",
        "tensor_b = torch.randn(3, 4) # Dimensione (3, 4)\n",
        "\n",
        "print(\"Tensor A:\\n\", tensor_a)\n",
        "print(\"Tensor B:\\n\", tensor_b)\n",
        "\n",
        "# 3. Esegui la moltiplicazione di matrici usando torch.matmul()\n",
        "matrix_product = torch.matmul(tensor_a, tensor_b)\n",
        "print(\"\\nMatrix Multiplication (torch.matmul) A @ B:\\n\", matrix_product)\n",
        "print(\"Shape of matrix_product:\", matrix_product.shape)\n",
        "\n",
        "# 4. Crea altri due tensori per l'operazione elemento per elemento\n",
        "tensor_c = torch.randn(2, 3)\n",
        "tensor_d = torch.randn(2, 3)\n",
        "\n",
        "print(\"\\nTensor C:\\n\", tensor_c)\n",
        "print(\"Tensor D:\\n\", tensor_d)\n",
        "\n",
        "# 5. Esegui la moltiplicazione elemento per elemento\n",
        "elementwise_product = tensor_c * tensor_d\n",
        "print(\"\\nElement-wise Multiplication C * D:\\n\", elementwise_product)\n",
        "\n",
        "# 6. Dimostra il broadcasting\n",
        "# Moltiplicazione per uno scalare\n",
        "scalar = 5.0\n",
        "broadcast_scalar = tensor_c * scalar\n",
        "print(\"\\nBroadcasting with a scalar (Tensor C * scalar):\\n\", broadcast_scalar)\n",
        "\n",
        "# Moltiplicazione con un tensore di dimensione (1, 3)\n",
        "tensor_e = torch.randn(1, 3) # Questo tensore verrà \"broadcastato\" sulla prima dimensione\n",
        "broadcast_tensor = tensor_c * tensor_e\n",
        "print(\"\\nBroadcasting with a (1,3) tensor (Tensor C * Tensor E):\\n\", broadcast_tensor)\n",
        "print(\"Shape of broadcast_tensor:\", broadcast_tensor.shape)"
      ],
      "id": "ce6bbfa4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1efdcaca"
      },
      "source": [
        "## Reshaping e Trasposizione PyTorch\n",
        "\n",
        "### Subtask:\n",
        "Aggiungi una cella di codice per mostrare come rimodellare (`reshape`) e trasporre (`transpose` o `.T`) tensori in PyTorch.\n"
      ],
      "id": "1efdcaca"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9ybCVXW6U4oz"
      },
      "id": "9ybCVXW6U4oz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cd5debb"
      },
      "source": [
        "questo codice dimostra come rimodellare e trasporre tensori in PyTorch. Vediamo i dettagli:\n",
        "\n",
        "Importazione della libreria PyTorch: Spiegazione della riga che importa la libreria torch.\n",
        "\n",
        "1) Creazione del tensore originale: Dettagli sulla creazione di tensor_original con torch.randn e la sua forma iniziale (2,3).\n",
        "2) Rimodellamento del tensore (reshape): Spiegazione delle due operazioni di reshape: una da (2,3) a (3,2) e l'altra da (2,3) a (6,). Verrà illustrato l'uso di .reshape() e torch.reshape().\n",
        "3) Trasposizione del tensore (transpose): Spiegazione delle due operazioni di trasposizione: usando la proprietà .T e la funzione torch.transpose(), dimostrando come scambiare le dimensioni del tensore (da (2,3) a (3,2)).\n",
        "\n",
        "4) Output e forme dei tensori: Spiegazione dell'uso di print per visualizzare i tensori risultanti e le loro dimensioni (.shape) in ogni passaggio.\n",
        "\n",
        "**Reasoning**:\n",
        "To demonstrate reshaping and transposing tensors in PyTorch, I need to add a code cell that imports torch, creates a sample tensor, then applies reshape and transpose operations, printing the tensor and its shape at each significant step as per the instructions.\n",
        "\n"
      ],
      "id": "6cd5debb"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MsDMIiTdUlbD"
      },
      "id": "MsDMIiTdUlbD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "647d8007"
      },
      "source": [
        "import torch\n",
        "\n",
        "# 2. Crea un tensore PyTorch di esempio\n",
        "tensor_original = torch.randn(2, 3)\n",
        "\n",
        "# 3. Stampa il tensore originale e la sua forma\n",
        "print(\"Tensore Originale:\\n\", tensor_original)\n",
        "print(\"Forma del Tensore Originale:\", tensor_original.shape)\n",
        "\n",
        "# 4. Rimodella il tensore in una nuova forma compatibile\n",
        "# Esempio 1: da (2,3) a (3,2)\n",
        "tensor_reshaped_1 = tensor_original.reshape(3, 2)\n",
        "print(\"\\nTensore Rimodellato (3,2):\\n\", tensor_reshaped_1)\n",
        "print(\"Forma del Tensore Rimodellato (3,2):\", tensor_reshaped_1.shape)\n",
        "\n",
        "# Esempio 2: da (2,3) a (6,)\n",
        "tensor_reshaped_2 = torch.reshape(tensor_original, (6,))\n",
        "print(\"\\nTensore Rimodellato (6,):\\n\", tensor_reshaped_2)\n",
        "print(\"Forma del Tensore Rimodellato (6,):\", tensor_reshaped_2.shape)\n",
        "\n",
        "# 6. Trasponi il tensore originale\n",
        "# Usando .T\n",
        "tensor_transposed_T = tensor_original.T\n",
        "print(\"\\nTensore Trasposto (.T):\\n\", tensor_transposed_T)\n",
        "print(\"Forma del Tensore Trasposto (.T):\", tensor_transposed_T.shape)\n",
        "\n",
        "# Usando torch.transpose()\n",
        "tensor_transposed_func = torch.transpose(tensor_original, 0, 1)\n",
        "print(\"\\nTensore Trasposto (torch.transpose):\\n\", tensor_transposed_func)\n",
        "print(\"Forma del Tensore Trasposto (torch.transpose):\", tensor_transposed_func.shape)"
      ],
      "id": "647d8007",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2a572f0"
      },
      "source": [
        "## Esempio Forward Propagation PyTorch\n",
        "\n",
        "### Subtask:\n",
        "Crea una cella di codice con un semplice esempio di forward propagation per un layer lineare con funzione di attivazione (es. ReLU o Sigmoid) in PyTorch.\n"
      ],
      "id": "b2a572f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8891c01c"
      },
      "source": [
        "Questo codice crea un semplice esempio di forward propagation (propagazione in avanti) in PyTorch. Questo è un concetto fondamentale nelle reti neurali, dove i dati di input vengono elaborati attraverso i diversi strati del modello per produrre un output. Ecco una spiegazione dettagliata delle diverse sezioni del codice:\n",
        "\n",
        "1) Importazione delle librerie: Spiegazione delle righe che importano torch e torch.nn, necessarie per la creazione e la gestione dei tensori e dei moduli di rete neurale in PyTorch.\n",
        "\n",
        "2) Definizione del Layer Lineare: Dettagli sulla creazione di un layer lineare (nn.Linear) con un numero specifico di features di input e output. Questo layer esegue una trasformazione lineare (moltiplicazione matriciale e aggiunta di un bias).\n",
        "\n",
        "3) Definizione della Funzione di Attivazione: Spiegazione della scelta e della creazione di una funzione di attivazione ReLU (nn.ReLU), che introduce non-linearità nel modello.\n",
        "\n",
        "4) Creazione del Tensore di Input: Dettagli sulla creazione di un tensore di input di esempio (input_tensor) con torch.randn, specificando la sua forma (dimensione del batch e numero di features).\n",
        "\n",
        "5) Esecuzione della Forward Propagation: Spiegazione dei passaggi sequenziali dell'input attraverso il layer lineare e successivamente attraverso la funzione di attivazione, inclusa l'osservazione delle forme dei tensori intermedi e finali.\n",
        "\n",
        "6) Output dei tensori e delle forme: Spiegazione dell'uso di print e .shape per visualizzare i tensori e le loro dimensioni in ogni passaggio chiave del processo di forward propagation.\n",
        "\n",
        "**Reasoning**:\n",
        "To demonstrate forward propagation in PyTorch, I need to add a code cell that sets up a linear layer, an activation function, an input tensor, and then performs the forward pass, printing the intermediate and final outputs along with their shapes.\n",
        "\n"
      ],
      "id": "8891c01c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e77808a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 1. Definisci un layer lineare\n",
        "input_features = 10\n",
        "output_features = 5\n",
        "linear_layer = nn.Linear(input_features, output_features)\n",
        "print(f\"Layer Lineare: {linear_layer}\\n\")\n",
        "\n",
        "# 2. Definisci una funzione di attivazione (es. ReLU)\n",
        "activation_function = nn.ReLU()\n",
        "print(f\"Funzione di Attivazione: {activation_function}\\n\")\n",
        "\n",
        "# 3. Crea un tensore di input di esempio\n",
        "# Batch di 1 elemento con 10 features\n",
        "input_tensor = torch.randn(1, input_features)\n",
        "print(f\"Tensore di Input:\\n{input_tensor}\\nShape: {input_tensor.shape}\\n\")\n",
        "\n",
        "# 4. Esegui la forward propagation\n",
        "# Passaggio attraverso il layer lineare\n",
        "output_linear = linear_layer(input_tensor)\n",
        "print(f\"Output dopo il Layer Lineare:\\n{output_linear}\\nShape: {output_linear.shape}\\n\")\n",
        "\n",
        "# Passaggio attraverso la funzione di attivazione\n",
        "output_activated = activation_function(output_linear)\n",
        "print(f\"Output dopo la Funzione di Attivazione (ReLU):\\n{output_activated}\\nShape: {output_activated.shape}\\n\")"
      ],
      "id": "0e77808a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45f1e752"
      },
      "source": [
        "## Visualizzazione della Forward Propagation in una Rete Neurale Semplice\n",
        "\n",
        "La forward propagation (o propagazione in avanti) è il processo attraverso il quale un input viene elaborato da una rete neurale per produrre un output. In questa sezione, vedremo un esempio dettagliato di come i dati fluiscono attraverso un singolo strato neurale, includendo la moltiplicazione per i pesi, l'aggiunta del bias e l'applicazione di una funzione di attivazione. Questo aiuterà a visualizzare il \"viaggio\" dei dati attraverso la rete.\n"
      ],
      "id": "45f1e752"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5ab8ddb"
      },
      "source": [
        "## Rappresentazione Schematica della Forward Propagation\n",
        "\n",
        "Procediamo con una descrizione più dettagliata e una rappresentazione schematica (in testo) di come i dati fluiscono attraverso i layer della nostra semplice rete neurale, basandoci sull'esempio numerico appena visto.\n",
        "\n",
        "### 1. Input Layer (dati di ingresso)\n",
        "\n",
        "I dati iniziano come un tensore di input. Nel nostro esempio, `input_data` è `[[1.0, 2.0]]`. Questo rappresenta un singolo esempio (batch size = 1) con due caratteristiche (features).\n",
        "\n",
        "```\n",
        "Input Layer (x)\n",
        "  [1.0, 2.0] --(2 features)\n",
        "```\n",
        "\n",
        "### 2. Linear Transformation (Moltiplicazione per i Pesi e Aggiunta del Bias)\n",
        "\n",
        "L'input viene poi moltiplicato per la matrice dei pesi (`W`) e viene aggiunto il vettore di bias (`b`). Questo costituisce la trasformazione lineare del layer denso.\n",
        "\n",
        "#### Dettaglio:\n",
        "*   **Pesi (W):** `[[1.0, 0.0, 2.0], [1.0, 3.0, 0.0]]` (Forma: 2x3)\n",
        "*   **Bias (b):** `[0.0, 1.0, 0.0]` (Forma: 3)\n",
        "\n",
        "Il calcolo è: `z = x @ W + b`\n",
        "\n",
        "#### Flusso:\n",
        "\n",
        "```\n",
        "Input (x)\n",
        " [1.0, 2.0]\n",
        "      |\n",
        "      V\n",
        "Layer Lineare (Wx + b = z)\n",
        "  (Moltiplicazione per i Pesi W)\n",
        "  (Aggiunta del Bias b)\n",
        "      |\n",
        "      V\n",
        "Output Lineare (z)\n",
        "  [3.0, 7.0, 2.0] --(3 neuroni/output)\n",
        "```\n",
        "\n",
        "### 3. Activation Function (Funzione di Attivazione)\n",
        "\n",
        "L'output della trasformazione lineare (`z`) passa attraverso una funzione di attivazione (nel nostro caso, ReLU). La funzione ReLU restituisce `max(0, z)` per ciascun elemento, introducendo non-linearità.\n",
        "\n",
        "#### Dettaglio:\n",
        "*   **ReLU:** `max(0, z)`\n",
        "\n",
        "#### Flusso:\n",
        "\n",
        "```\n",
        "Output Lineare (z)\n",
        " [3.0, 7.0, 2.0]\n",
        "      |\n",
        "      V\n",
        "Funzione di Attivazione (ReLU(z) = a)\n",
        "      |\n",
        "      V\n",
        "Output Attivato (a)\n",
        " [3.0, 7.0, 2.0] --(3 output attivati)\n",
        "```\n",
        "\n",
        "### Rappresentazione Completa del Flusso\n",
        "\n",
        "Qui una visione d'insieme del flusso di dati attraverso il singolo layer della rete:\n",
        "\n",
        "```\n",
        "+-----------------------+\n",
        "|      Input Layer (x)  |\n",
        "|     ([1.0, 2.0])      |\n",
        "+-----------+-----------+\n",
        "            |\n",
        "            V\n",
        "+-----------------------+\n",
        "|   Layer Lineare (Wx + b) |\n",
        "|   (Moltiplicazione con Pesi W) |\n",
        "|   (Aggiunta Bias b)   |\n",
        "+-----------+-----------+\n",
        "            |\n",
        "            V\n",
        "+-----------------------+\n",
        "|    Funzione di Attivazione    |\n",
        "|       (ReLU(z) = a)   |\n",
        "+-----------+-----------+\n",
        "            |\n",
        "            V\n",
        "+-----------------------+\n",
        "|      Output Finale    |\n",
        "|     ([3.0, 7.0, 2.0]) |\n",
        "+-----------------------+\n",
        "```\n",
        "\n",
        "Questo schema mostra che l'input `x` di 2 features viene trasformato dal layer denso in un output `z` di 3 neuroni, e successivamente attivato dalla funzione ReLU per produrre l'output finale `a` di 3 valori attivati. Ogni freccia rappresenta il passaggio dei dati attraverso una fase della forward propagation."
      ],
      "id": "b5ab8ddb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cef42a20",
        "outputId": "2823d373-74a4-4fa2-a68e-7cf1dfcb6b6a"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 1. Definisci un layer Denso (Fully Connected) con Keras\n",
        "# Useremo un layer con 2 neuroni in input e 3 neuroni in output\n",
        "# Per la dimostrazione, inizializzeremo i pesi e i bias manualmente\n",
        "# per rendere l'esempio più prevedibile.\n",
        "\n",
        "# Crea il layer Dense\n",
        "dense_layer = tf.keras.layers.Dense(units=3, use_bias=True, activation=None)\n",
        "\n",
        "# 2. Crea un tensore di input di esempio (batch size 1, 2 features)\n",
        "input_data = tf.constant([[1.0, 2.0]], dtype=tf.float32) # Usiamo valori interi ma dtype float per compatibilità\n",
        "print(\"Input Data (x):\\n\", input_data.numpy())\n",
        "print(\"Shape Input Data:\", input_data.shape)\n",
        "\n",
        "# 3. Il layer viene costruito alla prima chiamata. Eseguiamo una chiamata \"dummy\" per costruire il layer\n",
        "# e poter accedere a pesi e bias.\n",
        "_ = dense_layer(input_data)\n",
        "\n",
        "# 4. Accedi ai pesi e ai bias del layer dopo che è stato costruito\n",
        "# Normalmente questi vengono appresi durante l'addestramento, ma per la visualizzazione li impostiamo noi.\n",
        "manual_weights = tf.constant([[1.0, 0.0, 2.0],\n",
        "                              [1.0, 3.0, 0.0]], dtype=tf.float32) # (input_features, output_features)\n",
        "manual_bias = tf.constant([0.0, 1.0, 0.0], dtype=tf.float32) # (output_features,)\n",
        "\n",
        "dense_layer.set_weights([manual_weights, manual_bias])\n",
        "\n",
        "weights = dense_layer.get_weights()[0]\n",
        "bias = dense_layer.get_weights()[1]\n",
        "\n",
        "print(\"\\n--- Parametri del Layer --- \")\n",
        "print(\"Pesi (W):\\n\", weights)\n",
        "print(\"Shape Pesi:\", weights.shape)\n",
        "print(\"Bias (b):\\n\", bias)\n",
        "print(\"Shape Bias:\", bias.shape)\n",
        "\n",
        "# 5. Esegui la propagazione in avanti passo-passo\n",
        "print(\"\\n--- Forward Propagation --- \")\n",
        "\n",
        "# Passo 1: Moltiplicazione Input per Pesi (Wx)\n",
        "linear_output_before_bias = tf.matmul(input_data, weights)\n",
        "print(\"\\nOutput Lineare (Wx):\\n\", linear_output_before_bias.numpy())\n",
        "print(\"Shape Output Lineare (Wx):\", linear_output_before_bias.shape)\n",
        "\n",
        "# Passo 2: Aggiunta del Bias (Wx + b)\n",
        "linear_output = linear_output_before_bias + bias\n",
        "print(\"\\nOutput Lineare (Wx + b, z):\\n\", linear_output.numpy())\n",
        "print(\"Shape Output Lineare (z):\", linear_output.shape)\n",
        "\n",
        "# Passo 3: Applicazione della Funzione di Attivazione (ad esempio, ReLU)\n",
        "activation_function = tf.keras.activations.relu\n",
        "activated_output = activation_function(linear_output)\n",
        "print(\"\\nOutput Attivato (ReLU(z), a):\\n\", activated_output.numpy())\n",
        "print(\"Shape Output Attivato (a):\", activated_output.shape)\n",
        "\n",
        "print(\"\\n--- Flusso Completo (usando il layer Keras) --- \")\n",
        "# Questo è come il layer Keras esegue tutto in un colpo:\n",
        "kera_output = dense_layer(input_data)\n",
        "relu_keras_output = tf.keras.activations.relu(kera_output)\n",
        "print(\"Output del Layer Keras (z):\\n\", kera_output.numpy())\n",
        "print(\"Output Attivato del Layer Keras (a):\\n\", relu_keras_output.numpy())\n"
      ],
      "id": "cef42a20",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Data (x):\n",
            " [[1. 2.]]\n",
            "Shape Input Data: (1, 2)\n",
            "\n",
            "--- Parametri del Layer --- \n",
            "Pesi (W):\n",
            " [[1. 0. 2.]\n",
            " [1. 3. 0.]]\n",
            "Shape Pesi: (2, 3)\n",
            "Bias (b):\n",
            " [0. 1. 0.]\n",
            "Shape Bias: (3,)\n",
            "\n",
            "--- Forward Propagation --- \n",
            "\n",
            "Output Lineare (Wx):\n",
            " [[3. 6. 2.]]\n",
            "Shape Output Lineare (Wx): (1, 3)\n",
            "\n",
            "Output Lineare (Wx + b, z):\n",
            " [[3. 7. 2.]]\n",
            "Shape Output Lineare (z): (1, 3)\n",
            "\n",
            "Output Attivato (ReLU(z), a):\n",
            " [[3. 7. 2.]]\n",
            "Shape Output Attivato (a): (1, 3)\n",
            "\n",
            "--- Flusso Completo (usando il layer Keras) --- \n",
            "Output del Layer Keras (z):\n",
            " [[3. 7. 2.]]\n",
            "Output Attivato del Layer Keras (a):\n",
            " [[3. 7. 2.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d94c4708"
      },
      "source": [
        "## Backward Propagation PyTorch (Autograd)\n",
        "\n",
        "### Subtask:\n",
        "Aggiungi una cella di codice che mostri come calcolare i gradienti tramite autograd in PyTorch per una semplice funzione di perdita, evidenziando `.backward()` e `.grad`.\n"
      ],
      "id": "d94c4708"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "47mEzBZ0XJn5"
      },
      "id": "47mEzBZ0XJn5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb3bb617"
      },
      "source": [
        "Questo codice dimostra la propagazione all'indietro (backward propagation) o differenziazione automatica (autograd) in PyTorch. Questo meccanismo è fondamentale per l'addestramento delle reti neurali, poiché permette di calcolare i gradienti necessari per aggiornare i pesi del modello. Ecco una spiegazione dettagliata:\n",
        "\n",
        "1) Crea un tensore 'x' con requires_grad=True: Spiega la creazione del tensore x e l'importanza di requires_grad=True per abilitare il tracciamento delle operazioni ai fini del calcolo dei gradienti.\n",
        "2) Definisci una funzione 'y = x2'**: Descrivi la semplice operazione che trasforma x in y, che è x elevato al quadrato. Menziona come PyTorch tiene traccia di questa operazione.\n",
        "Definisci una funzione di perdita 'loss = y.sum()': Spiega come y.sum() viene utilizzata come una semplice funzione di perdita (scalarizzazione) per ottenere un singolo valore da cui calcolare il gradiente.\n",
        "\n",
        "3) Calcola i gradienti con .backward(): Dettaglia il funzionamento di loss.backward(), che innesca il calcolo automatico dei gradienti di loss rispetto a tutti i tensori per cui requires_grad=True era stato impostato (in questo caso, x).\n",
        "\n",
        "4) Stampa i gradienti di x (x.grad): Spiega come .grad su x restituisce i gradienti calcolati da backward(). Verrà fatto anche un riferimento alla verifica manuale dei gradienti inclusa nei commenti.\n",
        "\n",
        "\n",
        "**Reasoning**:\n",
        "I need to add a code cell to demonstrate backward propagation using PyTorch's autograd. This includes creating a tensor with `requires_grad=True`, defining a simple computation and loss, calling `.backward()`, and printing the gradients, as per the subtask instructions.\n",
        "\n"
      ],
      "id": "cb3bb617"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c418b2e"
      },
      "source": [
        "import torch\n",
        "\n",
        "# 1. Crea un tensore 'x' e imposta requires_grad=True per tracciare i gradienti\n",
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "print(f\"Tensore di input x: {x}\")\n",
        "\n",
        "# 2. Definisci una semplice funzione: y = x**2\n",
        "y = x**2\n",
        "print(f\"Output y (x**2): {y}\")\n",
        "\n",
        "# 3. Definisci una funzione di perdita semplice: loss = y.sum()\n",
        "loss = y.sum()\n",
        "print(f\"Funzione di perdita (y.sum()): {loss}\")\n",
        "\n",
        "# 4. Calcola i gradienti chiamando .backward() sulla perdita\n",
        "# Questo calcolerà d(loss)/dx per ciascun elemento di x\n",
        "loss.backward()\n",
        "\n",
        "# 5. Stampa i gradienti di x (x.grad)\n",
        "print(f\"Gradienti di x (x.grad): {x.grad}\")\n",
        "\n",
        "# Verifichiamo manualmente i gradienti:\n",
        "# loss = x[0]**2 + x[1]**2 + x[2]**2\n",
        "# d(loss)/dx[0] = 2*x[0] = 2*1.0 = 2.0\n",
        "# d(loss)/dx[1] = 2*x[1] = 2*2.0 = 4.0\n",
        "# d(loss)/dx[2] = 2*x[2] = 2*3.0 = 6.0"
      ],
      "id": "0c418b2e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb41dadd"
      },
      "source": [
        "## Concatenazione e Stacking PyTorch\n",
        "\n",
        "### Subtask:\n",
        "Includi una cella di codice che illustra come concatenare (`torch.cat`) e impilare (`torch.stack`) tensori in PyTorch.\n"
      ],
      "id": "eb41dadd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe210443"
      },
      "source": [
        "Questo codice illustra come concatenare (torch.cat) e impilare (torch.stack) tensori in PyTorch, operazioni fondamentali per manipolare i dati. Ecco una spiegazione dettagliata:\n",
        "\n",
        "1) Importazione della libreria PyTorch: Spiegazione della riga che importa la libreria torch.\n",
        "\n",
        "2) Creazione di tensori di esempio: Dettagli sulla creazione di tensor1, tensor2, tensor3, tensor_cat_col1 e tensor_cat_col2 con torch.randn e le loro forme, evidenziando la loro compatibilità per le diverse operazioni.\n",
        "\n",
        "3) Concatenazione lungo la dimensione 0 (righe): Spiegazione dell'uso di torch.cat((tensor1, tensor2), dim=0), le condizioni di compatibilità delle dimensioni e come influisce sulla forma del tensore risultante.\n",
        "\n",
        "4) Concatenazione lungo la dimensione 1 (colonne): Spiegazione dell'uso di torch.cat((tensor_cat_col1, tensor_cat_col2), dim=1), le condizioni di compatibilità (specialmente per le dimensioni non concatenate) e la forma del risultato.\n",
        "\n",
        "5) Impilamento lungo una nuova dimensione (dim=0): Dettagli sull'uso di torch.stack((tensor1, tensor2), dim=0), sottolineando che torch.stack crea una nuova dimensione, e la forma del tensore impilato.\n",
        "\n",
        "5) Impilamento lungo una nuova dimensione (dim=1): Spiegazione dell'uso di torch.stack((tensor1, tensor2), dim=1) per impilare lungo una dimensione differente, e la forma risultante.\n",
        "\n",
        "6) Output e forme dei tensori: Spiegazione dell'uso di print e .shape per visualizzare i tensori risultanti e le loro dimensioni in ogni passaggio chiave.\n",
        "\n",
        "**Reasoning**:\n",
        "To fulfill the subtask, I need to add a code cell that imports torch, creates example tensors with compatible dimensions, demonstrates both `torch.cat` along different dimensions and `torch.stack` along a new dimension, and prints the resulting tensors and their shapes, as per the instructions.\n",
        "\n"
      ],
      "id": "fe210443"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac77d392",
        "outputId": "26302e38-d635-45d4-8d04-a33e867a9460"
      },
      "source": [
        "import torch\n",
        "\n",
        "# 1. Crea due tensori PyTorch di esempio\n",
        "print(\"\\n\\n 1 Creazione (torch.randn) di 3 tensori\")\n",
        "print(\"-\"*60)\n",
        "tensor1 = torch.randn(2, 3) # Tensore di dimensione (2, 3)\n",
        "tensor2 = torch.randn(2, 3) # Tensore di dimensione (2, 3)\n",
        "tensor3 = torch.randn(3, 3) # Tensore di dimensione (3, 3) per concatenazione lungo dim=0\n",
        "\n",
        "print(\"Tensore 1:\\n\", tensor1)\n",
        "print(\"Tensore 2:\\n\", tensor2)\n",
        "print(\"Tensore 3:\\n\", tensor3)\n",
        "\n",
        "# 2. Concatena i tensori lungo la dimensione 0 (righe)\n",
        "# I tensori devono avere le stesse dimensioni tranne che lungo la dimensione di concatenazione\n",
        "print(\"\\n\\n 2  Concatena (torch.cat) lungo la dim 0 cioè le righe (uno sull'altro)\")\n",
        "print(\"-\"*60)\n",
        "concatenated_dim0 = torch.cat((tensor1, tensor2), dim=0)\n",
        "print(\"\\nConcatenazione lungo dim=0 (righe):\\n\", concatenated_dim0)\n",
        "print(\"\\nForma dopo concatenazione dim=0:\", concatenated_dim0.shape)\n",
        "\n",
        "# 3. Concatena i tensori lungo la dimensione 1 (colonne)\n",
        "# I tensori devono avere le stesse dimensioni tranne che lungo la dimensione di concatenazione\n",
        "# Usiamo tensor1 e tensor3 per dimostrare, tensor3 ha più righe ma le stesse colonne\n",
        "# NOTA: Per concatenare lungo dim=1, le dimensioni *non* concatenate devono essere le stesse.\n",
        "# Quindi useremo due nuovi tensori con le righe uguali per questa dimostrazione.\n",
        "print(\"\\n\\n 3 Concatena (torch.cat dim1) lungo la dim 1 cioè le colonne (uno accanto all'altro)\")\n",
        "print(\"-\"*60)\n",
        "concatenated_dim1 = torch.cat((tensor1, tensor2), dim=1)\n",
        "print(\"\\nConcatenazione lungo dim=1 (colonne):\\n\", concatenated_dim1)\n",
        "print(\"\\nForma dopo concatenazione dim=1:\", concatenated_dim1.shape)\n",
        "\n",
        "# 4. Impila i tensori lungo una nuova dimensione (dim=0)\n",
        "# I tensori devono avere la stessa dimensione per essere impilati.\n",
        "print(\"\\n\\n 4  Impila (torch.stack dim0) i tensori sulla 3 dimensione (tridimensionale) uno dopo l'altro\")\n",
        "print(\"-\"*60)\n",
        "stacked_dim0 = torch.stack((tensor1, tensor2), dim=0)\n",
        "print(\"\\nImpilamento lungo dim=0 (nuova dimensione):\\n\", stacked_dim0)\n",
        "print(\"\\nForma dopo impilamento dim=0:\", stacked_dim0.shape)\n",
        "\n",
        "# 5. Impila i tensori lungo una nuova dimensione (dim=1)\n",
        "# Anche qui, i tensori devono avere la stessa dimensione.\n",
        "print(\"\\n\\n 5 Impila i tensori (torch.stack dim1) sulla 3 dimensione (tridimensionale) uno dopo l'altro \")\n",
        "print(\"-\"*60)\n",
        "stacked_dim1 = torch.stack((tensor1, tensor2), dim=1)\n",
        "print(\"\\nImpilamento lungo dim=1 (nuova dimensione):\\n\", stacked_dim1)\n",
        "print(\"\\nForma dopo impilamento dim=1:\", stacked_dim1.shape)"
      ],
      "id": "ac77d392",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " 1 Creazione (torch.randn) di 3 tensori\n",
            "------------------------------------------------------------\n",
            "Tensore 1:\n",
            " tensor([[ 0.2136,  0.2045, -0.6484],\n",
            "        [ 0.1167, -0.3811,  0.8691]])\n",
            "Tensore 2:\n",
            " tensor([[ 0.2363, -0.3905,  1.7381],\n",
            "        [-1.8129, -0.9232,  0.2142]])\n",
            "Tensore 3:\n",
            " tensor([[ 0.5595, -0.5102,  0.4872],\n",
            "        [ 1.0013, -0.8833, -0.6030],\n",
            "        [ 0.2029, -0.1399, -0.5093]])\n",
            "\n",
            "\n",
            " 2  Concatena (torch.cat) lungo la dim 0 cioè le righe (uno sull'altro)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Concatenazione lungo dim=0 (righe):\n",
            " tensor([[ 0.2136,  0.2045, -0.6484],\n",
            "        [ 0.1167, -0.3811,  0.8691],\n",
            "        [ 0.2363, -0.3905,  1.7381],\n",
            "        [-1.8129, -0.9232,  0.2142]])\n",
            "\n",
            "Forma dopo concatenazione dim=0: torch.Size([4, 3])\n",
            "\n",
            "\n",
            " 3 Concatena (torch.cat dim1) lungo la dim 1 cioè le colonne (uno accanto all'altro)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Concatenazione lungo dim=1 (colonne):\n",
            " tensor([[ 0.2136,  0.2045, -0.6484,  0.2363, -0.3905,  1.7381],\n",
            "        [ 0.1167, -0.3811,  0.8691, -1.8129, -0.9232,  0.2142]])\n",
            "\n",
            "Forma dopo concatenazione dim=1: torch.Size([2, 6])\n",
            "\n",
            "\n",
            " 4  Impila (torch.stack dim0) i tensori sulla 3 dimensione (tridimensionale) uno dopo l'altro\n",
            "------------------------------------------------------------\n",
            "\n",
            "Impilamento lungo dim=0 (nuova dimensione):\n",
            " tensor([[[ 0.2136,  0.2045, -0.6484],\n",
            "         [ 0.1167, -0.3811,  0.8691]],\n",
            "\n",
            "        [[ 0.2363, -0.3905,  1.7381],\n",
            "         [-1.8129, -0.9232,  0.2142]]])\n",
            "\n",
            "Forma dopo impilamento dim=0: torch.Size([2, 2, 3])\n",
            "\n",
            "\n",
            " 5 Impila i tensori (torch.stack dim1) sulla 3 dimensione (tridimensionale) uno dopo l'altro \n",
            "------------------------------------------------------------\n",
            "\n",
            "Impilamento lungo dim=1 (nuova dimensione):\n",
            " tensor([[[ 0.2136,  0.2045, -0.6484],\n",
            "         [ 0.2363, -0.3905,  1.7381]],\n",
            "\n",
            "        [[ 0.1167, -0.3811,  0.8691],\n",
            "         [-1.8129, -0.9232,  0.2142]]])\n",
            "\n",
            "Forma dopo impilamento dim=1: torch.Size([2, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d3943bb"
      },
      "source": [
        "## Operazioni di Riduzione PyTorch\n",
        "\n",
        "### Subtask:\n",
        "Aggiungi una cella di codice che dimostri operazioni di riduzione come somma (torch.sum), media (torch.mean) e massimo (torch.max) su tensori PyTorch.\n"
      ],
      "id": "2d3943bb"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dxG0kW5bniuA"
      },
      "id": "dxG0kW5bniuA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a2a319b"
      },
      "source": [
        "Questo codice dimostra le operazioni di riduzione fondamentali su tensori PyTorch, come la somma, la media e il massimo, sia a livello globale che lungo specifiche dimensioni. Ecco una spiegazione dettagliata:\n",
        "\n",
        "1) Importazione della libreria PyTorch: Spiegazione della riga che importa la libreria torch.\n",
        "\n",
        "2) Creazione di un tensore di esempio: Dettagli sulla creazione di tensor_data con torch.randn e la sua forma iniziale (3x4), e la stampa del tensore originale e della sua forma.\n",
        "\n",
        "3) Calcolo della somma totale: Spiegazione dell'uso di torch.sum(tensor_data) per calcolare la somma di tutti gli elementi del tensore, e la stampa del risultato.\n",
        "\n",
        "4) Calcolo della somma lungo dimensioni specifiche: Dettagli sull'uso di torch.sum(tensor_data, dim=0) per la somma lungo le colonne e torch.sum(tensor_data, dim=1) per la somma lungo le righe, inclusa la spiegazione delle forme risultanti.\n",
        "\n",
        "5) Calcolo della media totale: Spiegazione dell'uso di torch.mean(tensor_data) per calcolare la media di tutti gli elementi del tensore, e la stampa del risultato.\n",
        "\n",
        "6) Calcolo della media lungo dimensioni specifiche: Dettagli sull'uso di torch.mean(tensor_data, dim=0) per la media lungo le colonne e torch.mean(tensor_data, dim=1) per la media lungo le righe, inclusa la spiegazione delle forme risultanti.\n",
        "\n",
        "7) Calcolo del valore massimo totale: Spiegazione dell'uso di torch.max(tensor_data) per trovare il valore massimo tra tutti gli elementi del tensore, e la stampa del risultato.\n",
        "\n",
        "8) Calcolo del valore massimo lungo dimensioni specifiche: Dettagli sull'uso di torch.max(tensor_data, dim=0) e torch.max(tensor_data, dim=1) per trovare i valori massimi e i relativi indici lungo le colonne e le righe, inclusa la spiegazione delle forme risultanti per valori e indici.\n",
        "\n",
        "**Reasoning**:\n",
        "To fulfill the subtask, I need to add a code cell that imports torch, creates a sample tensor, then applies sum, mean, and max reduction operations globally and along specified dimensions (dim=0 and dim=1), and prints the tensor and the results of each operation along with their shapes, as per the instructions.\n",
        "\n"
      ],
      "id": "4a2a319b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "300cb9e6"
      },
      "source": [
        "import torch\n",
        "\n",
        "# 1. Crea un tensore PyTorch di esempio (ad esempio, di dimensione 3x4)\n",
        "tensor_data = torch.randn(3, 4)\n",
        "print(\"Tensore Originale:\\n\", tensor_data)\n",
        "print(\"Forma del Tensore Originale:\", tensor_data.shape)\n",
        "\n",
        "# 2. Calcola la somma di tutti gli elementi del tensore\n",
        "total_sum = torch.sum(tensor_data)\n",
        "print(\"\\nSomma totale del tensore:\", total_sum.item())\n",
        "\n",
        "# 3. Calcola la somma lungo la dimensione 0 (righe) e lungo la dimensione 1 (colonne)\n",
        "sum_dim0 = torch.sum(tensor_data, dim=0)\n",
        "print(\"\\nSomma lungo la dimensione 0 (colonne):\\n\", sum_dim0)\n",
        "print(\"Forma della somma dim 0:\", sum_dim0.shape)\n",
        "\n",
        "sum_dim1 = torch.sum(tensor_data, dim=1)\n",
        "print(\"\\nSomma lungo la dimensione 1 (righe):\\n\", sum_dim1)\n",
        "print(\"Forma della somma dim 1:\", sum_dim1.shape)\n",
        "\n",
        "# 4. Calcola la media di tutti gli elementi del tensore\n",
        "total_mean = torch.mean(tensor_data)\n",
        "print(\"\\nMedia totale del tensore:\", total_mean.item())\n",
        "\n",
        "# 5. Calcola la media lungo la dimensione 0 e lungo la dimensione 1\n",
        "mean_dim0 = torch.mean(tensor_data, dim=0)\n",
        "print(\"\\nMedia lungo la dimensione 0 (colonne):\\n\", mean_dim0)\n",
        "print(\"Forma della media dim 0:\", mean_dim0.shape)\n",
        "\n",
        "mean_dim1 = torch.mean(tensor_data, dim=1)\n",
        "print(\"\\nMedia lungo la dimensione 1 (righe):\\n\", mean_dim1)\n",
        "print(\"Forma della media dim 1:\", mean_dim1.shape)\n",
        "\n",
        "# 6. Trova il valore massimo di tutti gli elementi del tensore\n",
        "total_max = torch.max(tensor_data)\n",
        "print(\"\\nValore massimo totale del tensore:\", total_max.item())\n",
        "\n",
        "# 7. Trova il valore massimo e i relativi indici lungo la dimensione 0 e lungo la dimensione 1\n",
        "max_dim0 = torch.max(tensor_data, dim=0)\n",
        "print(\"\\nValore massimo lungo la dimensione 0 (colonne):\\n\", max_dim0.values)\n",
        "print(\"Indici del massimo lungo la dimensione 0:\\n\", max_dim0.indices)\n",
        "print(\"Forma del massimo dim 0:\", max_dim0.values.shape)\n",
        "\n",
        "max_dim1 = torch.max(tensor_data, dim=1)\n",
        "print(\"\\nValore massimo lungo la dimensione 1 (righe):\\n\", max_dim1.values)\n",
        "print(\"Indici del massimo lungo la dimensione 1:\\n\", max_dim1.indices)\n",
        "print(\"Forma del massimo dim 1:\", max_dim1.values.shape)"
      ],
      "id": "300cb9e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63a5deef"
      },
      "source": [
        "## Broadcasting TensorFlow\n",
        "\n",
        "### Subtask:\n",
        "Aggiungi una cella di codice per dimostrare esempi più complessi di broadcasting con tensori TensorFlow.\n"
      ],
      "id": "63a5deef"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5666917e"
      },
      "source": [
        "Questo codice dimostra esempi più complessi di broadcasting con tensori TensorFlow, mostrando come TensorFlow adatta automaticamente le forme dei tensori per rendere possibili le operazioni. Ecco una spiegazione dettagliata:\n",
        "\n",
        "1) Importazione della libreria TensorFlow: Spiegazione della riga che importa la libreria tensorflow con l'alias tf.\n",
        "\n",
        "2) Creazione di 'tensor_a': Dettagli sulla creazione di tensor_a con tf.constant e la sua forma (4, 1, 3), specificando il tipo di dato tf.float32.\n",
        "\n",
        "3) Creazione di 'tensor_b': Dettagli sulla creazione di tensor_b con tf.constant e la sua forma (1, 5, 3), specificando il tipo di dato tf.float32. Viene evidenziato che le forme di tensor_a e tensor_b sono compatibili per il broadcasting.\n",
        "\n",
        "4) Addizione con broadcasting (tensor_a + tensor_b): Spiegazione di come TensorFlow estende implicitamente la dimensione 1 di tensor_a da 1 a 5, e la dimensione 0 di tensor_b da 1 a 4, per rendere le forme compatibili per l'addizione. Verrà spiegato che il risultato ha forma (4, 5, 3).\n",
        "Creazione di 'tensor_c' e 'tensor_d' per un secondo esempio: Dettagli sulla creazione di tensor_c con forma (5, 1) e tensor_d con forma (1, 3) usando tf.random.normal, per dimostrare un altro scenario di broadcasting.\n",
        "\n",
        "5) Moltiplicazione con broadcasting (tensor_c * tensor_d): Spiegazione di come TensorFlow applica le regole di broadcasting per la moltiplicazione di tensor_c e tensor_d, estendendo le dimensioni appropriate per ottenere un risultato di forma (5, 3).\n",
        "\n",
        "6) Output dei tensori e delle forme: Spiegazione dell'uso di print per visualizzare i tensori creati, i risultati delle operazioni e le loro dimensioni (.shape) in ogni passaggio.\n",
        "\n",
        "**Reasoning**:\n",
        "I need to add a code cell to demonstrate complex TensorFlow broadcasting examples as per the instructions, including creating tensors with specific shapes, performing an operation that leverages broadcasting, and printing the tensors and their shapes.\n",
        "\n"
      ],
      "id": "5666917e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2b7b493"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Crea un tensore di esempio 'tensor_a' con forma (4, 1, 3)\n",
        "tensor_a = tf.constant([[[1, 2, 3]],\n",
        "                        [[4, 5, 6]],\n",
        "                        [[7, 8, 9]],\n",
        "                        [[10, 11, 12]]], dtype=tf.float32)\n",
        "print(\"Tensor A:\\n\", tensor_a)\n",
        "print(\"Forma di Tensor A:\", tensor_a.shape)\n",
        "\n",
        "# 2. Crea un altro tensore di esempio 'tensor_b' con forma compatibile per il broadcasting (1, 5, 3)\n",
        "tensor_b = tf.constant([[[100, 200, 300],\n",
        "                         [10, 20, 30],\n",
        "                         [1, 2, 3],\n",
        "                         [0.1, 0.2, 0.3],\n",
        "                         [0, 0, 0]]], dtype=tf.float32)\n",
        "print(\"\\nTensor B:\\n\", tensor_b)\n",
        "print(\"Forma di Tensor B:\", tensor_b.shape)\n",
        "\n",
        "# 3. Esegui un'operazione che sfrutti il broadcasting (es. addizione)\n",
        "# TensorFlow \"estenderà\" la dimensione 1 di tensor_a da 1 a 5\n",
        "# e la dimensione 0 di tensor_b da 1 a 4.\n",
        "# Il risultato sarà un tensore di forma (4, 5, 3).\n",
        "broadcasted_sum = tensor_a + tensor_b\n",
        "\n",
        "print(\"\\nRisultato dell'addizione con broadcasting (Tensor A + Tensor B):\\n\", broadcasted_sum)\n",
        "print(\"Forma del risultato del broadcasting:\", broadcasted_sum.shape)\n",
        "\n",
        "# Esempio di broadcasting con moltiplicazione\n",
        "tensor_c = tf.random.normal((5, 1))\n",
        "tensor_d = tf.random.normal((1, 3))\n",
        "print(\"\\nTensor C (5,1):\\n\", tensor_c)\n",
        "print(\"Forma di Tensor C:\", tensor_c.shape)\n",
        "print(\"\\nTensor D (1,3):\\n\", tensor_d)\n",
        "print(\"Forma di Tensor D:\", tensor_d.shape)\n",
        "\n",
        "broadcasted_mul = tensor_c * tensor_d\n",
        "print(\"\\nRisultato della moltiplicazione con broadcasting (Tensor C * Tensor D):\\n\", broadcasted_mul)\n",
        "print(\"Forma del risultato del broadcasting (moltiplicazione):\", broadcasted_mul.shape)\n"
      ],
      "id": "b2b7b493",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfb6fe95"
      },
      "source": [
        "## Esempio Layer Denso TensorFlow (Forward)\n",
        "\n",
        "### Subtask:\n",
        "Crea una cella di codice con un esempio di forward propagation utilizzando un layer `tf.keras.layers.Dense` e una funzione di attivazione in TensorFlow.\n"
      ],
      "id": "dfb6fe95"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "562810ec"
      },
      "source": [
        "**Reasoning**:\n",
        "To demonstrate forward propagation in TensorFlow using a Dense layer and an activation function, I will add a code cell that imports the necessary libraries, defines the layers and an input tensor, and then performs the forward pass while printing the shapes at each stage, as per the instructions.\n",
        "\n"
      ],
      "id": "562810ec"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5fb188d"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# 1. Definisci un layer Denso\n",
        "input_features = 10\n",
        "output_features = 5\n",
        "dense_layer = layers.Dense(output_features, input_shape=(input_features,))\n",
        "print(f\"Layer Denso: {dense_layer}\\n\")\n",
        "\n",
        "# 2. Definisci una funzione di attivazione (es. ReLU)\n",
        "activation_function = layers.ReLU()\n",
        "print(f\"Funzione di Attivazione: {activation_function}\\n\")\n",
        "\n",
        "# 3. Crea un tensore di input di esempio\n",
        "# Batch di 1 elemento con 10 features\n",
        "input_tensor = tf.random.normal((1, input_features))\n",
        "print(f\"Tensore di Input:\\n{input_tensor}\\nShape: {input_tensor.shape}\\n\")\n",
        "\n",
        "# 4. Esegui la forward propagation\n",
        "# Passaggio attraverso il layer denso\n",
        "output_dense = dense_layer(input_tensor)\n",
        "print(f\"Output dopo il Layer Denso:\\n{output_dense}\\nShape: {output_dense.shape}\\n\")\n",
        "\n",
        "# 5. Passaggio attraverso la funzione di attivazione\n",
        "output_activated = activation_function(output_dense)\n",
        "print(f\"Output dopo la Funzione di Attivazione (ReLU):\\n{output_activated}\\nShape: {output_activated.shape}\\n\")"
      ],
      "id": "e5fb188d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d56d713d"
      },
      "source": [
        "Questo codice crea un semplice esempio di forward propagation (propagazione in avanti) utilizzando un layer tf.keras.layers.Dense e una funzione di attivazione in TensorFlow. Ecco una spiegazione dettagliata:\n",
        "\n",
        "1) Importazione delle librerie: Spiegazione delle righe che importano tensorflow e tensorflow.keras.layers, necessarie per la creazione e la gestione dei tensori e dei layer di rete neurale in TensorFlow.\n",
        "\n",
        "2) Definizione del Layer Denso: Dettagli sulla creazione di un layer denso (layers.Dense) con un numero specifico di neuroni di output. Verrà sottolineato che, a differenza dell'esempio precedente, l'argomento input_shape non è stato specificato per evitare avvisi, e il layer verrà costruito automaticamente alla prima chiamata.\n",
        "\n",
        "3) Definizione della Funzione di Attivazione: Spiegazione della scelta e della creazione di una funzione di attivazione ReLU (layers.ReLU), che introduce non-linearità nel modello.\n",
        "\n",
        "4) Creazione del Tensore di Input: Dettagli sulla creazione di un tensore di input di esempio (input_tensor) con tf.random.normal, specificando la sua forma (dimensione del batch e numero di features).\n",
        "\n",
        "5) Esecuzione della Forward Propagation: Spiegazione dei passaggi sequenziali dell'input attraverso il layer denso e successivamente attraverso la funzione di attivazione, inclusa l'osservazione delle forme dei tensori intermedi e finali. Verrà ribadito che il layer dense_layer si 'costruisce' alla sua prima invocazione con l'input.\n",
        "\n",
        "6) Output dei tensori e delle forme: Spiegazione dell'uso di print per visualizzare i tensori e le loro dimensioni (.shape) in ogni passaggio chiave del processo di forward propagation.\n",
        "\n",
        "**Reasoning**:\n",
        "The previous code generated a UserWarning regarding the `input_shape` argument in `tf.keras.layers.Dense`. To address this, I will remove `input_shape` from the `Dense` layer constructor. The layer will then be built automatically upon its first call, which is a common and accepted practice in TensorFlow and prevents the warning without altering the intended functionality of the forward propagation example.\n",
        "\n"
      ],
      "id": "d56d713d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caa83973"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# 1. Definisci un layer Denso\n",
        "input_features = 10\n",
        "output_features = 5\n",
        "dense_layer = layers.Dense(output_features)\n",
        "print(f\"Layer Denso: {dense_layer}\\n\")\n",
        "\n",
        "# 2. Definisci una funzione di attivazione (es. ReLU)\n",
        "activation_function = layers.ReLU()\n",
        "print(f\"Funzione di Attivazione: {activation_function}\\n\")\n",
        "\n",
        "# 3. Crea un tensore di input di esempio\n",
        "# Batch di 1 elemento con 10 features\n",
        "input_tensor = tf.random.normal((1, input_features))\n",
        "print(f\"Tensore di Input:\\n{input_tensor}\\nShape: {input_tensor.shape}\\n\")\n",
        "\n",
        "# 4. Esegui la forward propagation\n",
        "# Passaggio attraverso il layer denso. Il layer verrà costruito alla prima chiamata.\n",
        "output_dense = dense_layer(input_tensor)\n",
        "print(f\"Output dopo il Layer Denso:\\n{output_dense}\\nShape: {output_dense.shape}\\n\")\n",
        "\n",
        "# 5. Passaggio attraverso la funzione di attivazione\n",
        "output_activated = activation_function(output_dense)\n",
        "print(f\"Output dopo la Funzione di Attivazione (ReLU):\\n{output_activated}\\nShape: {output_activated.shape}\\n\")"
      ],
      "id": "caa83973",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdc9ca9f"
      },
      "source": [
        "## Differenziazione con GradientTape TensorFlow\n",
        "\n",
        "### Subtask:\n",
        "Aggiungi una cella di codice per dimostrare la differenziazione automatica di una funzione utilizzando `tf.GradientTape` in TensorFlow.\n"
      ],
      "id": "cdc9ca9f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9997f4e2"
      },
      "source": [
        "Questo codice dimostra la differenziazione automatica di una funzione utilizzando tf.GradientTape in TensorFlow, un meccanismo fondamentale per l'addestramento delle reti neurali. Ecco una spiegazione dettagliata:\n",
        "\n",
        "1) Definizione di una variabile TensorFlow 'x': Spiegazione della creazione della variabile x con tf.Variable e perché è cruciale per il tracciamento automatico dei gradienti in TensorFlow.\n",
        "\n",
        "2) Utilizzo del blocco tf.GradientTape(): Dettagli sull'uso di with tf.GradientTape() as tape: per registrare tutte le operazioni eseguite al suo interno, che saranno poi utilizzate per calcolare i gradienti.\n",
        "Definizione della funzione y = x * x:\n",
        "\n",
        "3) Spiegazione della semplice funzione matematica definita, y come il quadrato di x, e come le operazioni sono monitorate dal GradientTape.\n",
        "\n",
        "4) Calcolo del gradiente con tape.gradient(): Descrizione di come tape.gradient(y, x) viene utilizzato per calcolare la derivata di y rispetto a x (dy/dx).\n",
        "\n",
        "5) Stampa del gradiente calcolato: Spiegazione di come accedere e stampare il valore numerico del gradiente calcolato (dy_dx.numpy()), con un riferimento alla verifica manuale inclusa nei commenti.\n",
        "\n",
        "**Reasoning**:\n",
        "To demonstrate automatic differentiation using `tf.GradientTape` in TensorFlow, I will add a code cell that initializes a `tf.Variable`, defines a function, and then computes and prints its gradient.\n",
        "\n"
      ],
      "id": "9997f4e2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5432a1c4"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Definisci una variabile TensorFlow 'x'\n",
        "x = tf.Variable(3.0)\n",
        "print(f\"Variabile x: {x.numpy()}\")\n",
        "\n",
        "# 2. Utilizza un blocco tf.GradientTape() per registrare le operazioni\n",
        "with tf.GradientTape() as tape:\n",
        "    # 3. Definisci una funzione y che dipenda da x (es. y = x * x)\n",
        "    y = x * x\n",
        "    print(f\"Funzione y (x * x): {y.numpy()}\")\n",
        "\n",
        "# 4. Calcola il gradiente di y rispetto a x\n",
        "dy_dx = tape.gradient(y, x)\n",
        "\n",
        "# 5. Stampa il gradiente calcolato\n",
        "print(f\"Gradiente di y rispetto a x (dy/dx): {dy_dx.numpy()}\")\n",
        "\n",
        "# Verifichiamo manualmente il gradiente:\n",
        "# Se y = x^2, allora dy/dx = 2x.\n",
        "# Per x = 3.0, dy/dx dovrebbe essere 2 * 3.0 = 6.0"
      ],
      "id": "5432a1c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "defda903"
      },
      "source": [
        "## Operazioni Matrice Complesse TensorFlow\n",
        "\n",
        "### Subtask:\n",
        "Aggiungi una cella di codice che esplora operazioni matriciali più complesse in TensorFlow, come il prodotto di Hadamard (elemento per elemento) e l'inversione di matrici (se applicabile).\n"
      ],
      "id": "defda903"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a48d39a"
      },
      "source": [
        "Questo codice esplora operazioni matriciali più complesse in TensorFlow, come il prodotto di Hadamard (moltiplicazione elemento per elemento) e l'inversione di matrici. Ecco una spiegazione dettagliata:\n",
        "\n",
        "1) Importazione della libreria TensorFlow: Spiegazione della riga che importa la libreria tensorflow con l'alias tf.\n",
        "\n",
        "2) Prodotto di Hadamard (Moltiplicazione elemento per elemento): Dettagli sulla creazione di due tensori di esempio (tensor_hadamard_a e tensor_hadamard_b) e come l'operatore * esegue il prodotto di Hadamard, stampando i tensori e il risultato con le loro forme.\n",
        "3) Inversione di Matrici: Spiegazione della creazione di una matrice invertibile (matrix_to_invert) e l'uso di tf.linalg.inv per calcolarne l'inversa. Verrà mostrato come verificare l'inversione moltiplicando la matrice originale per la sua inversa per ottenere una matrice identità.\n",
        "\n",
        "4) Gestione degli errori per matrici non invertibili: Dimostrazione di come un blocco try-except gestisce l'errore tf.errors.InvalidArgumentError quando si tenta di invertire una matrice singolare (non invertibile), come quella con determinante nullo.\n",
        "\n",
        "5) Output dei tensori e delle forme: Spiegazione dell'uso di print per visualizzare i tensori creati, i risultati delle operazioni e le loro dimensioni (.shape) in ogni passaggio.\n",
        "\n",
        "**Reasoning**:\n",
        "To fulfill the subtask, I need to add a code cell that demonstrates Hadamard product and matrix inversion in TensorFlow. This includes importing TensorFlow, creating example tensors, performing element-wise multiplication, attempting matrix inversion, and verifying the result with a try-except block for non-invertible matrices.\n",
        "\n"
      ],
      "id": "7a48d39a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12c8fce6"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Prodotto di Hadamard (Element-wise multiplication)\n",
        "print(\"--- Prodotto di Hadamard ---\")\n",
        "# Crea due tensori di esempio di uguale dimensione (es. 3x3)\n",
        "tensor_hadamard_a = tf.constant([[1, 2, 3],\n",
        "                               [4, 5, 6],\n",
        "                               [7, 8, 9]], dtype=tf.float32)\n",
        "tensor_hadamard_b = tf.constant([[10, 20, 30],\n",
        "                               [40, 50, 60],\n",
        "                               [70, 80, 90]], dtype=tf.float32)\n",
        "\n",
        "print(\"Tensor Hadamard A:\\n\", tensor_hadamard_a.numpy())\n",
        "print(\"Forma di Tensor Hadamard A:\", tensor_hadamard_a.shape)\n",
        "print(\"Tensor Hadamard B:\\n\", tensor_hadamard_b.numpy())\n",
        "print(\"Forma di Tensor Hadamard B:\", tensor_hadamard_b.shape)\n",
        "\n",
        "# Calcola il prodotto di Hadamard (moltiplicazione elemento per elemento)\n",
        "hadamard_product = tensor_hadamard_a * tensor_hadamard_b\n",
        "\n",
        "print(\"\\nRisultato del Prodotto di Hadamard (A * B):\\n\", hadamard_product.numpy())\n",
        "print(\"Forma del Prodotto di Hadamard:\", hadamard_product.shape)\n",
        "\n",
        "# 2. Inversione di Matrici\n",
        "print(\"\\n--- Inversione di Matrici ---\")\n",
        "\n",
        "# Crea un tensore quadrato invertibile (es. 3x3)\n",
        "# Usiamo tf.eye(3) per una matrice invertibile semplice e poi la modifichiamo leggermente\n",
        "matrix_to_invert = tf.constant([[1.0, 2.0, 3.0],\n",
        "                                [0.0, 1.0, 4.0],\n",
        "                                [5.0, 6.0, 0.0]], dtype=tf.float32)\n",
        "\n",
        "print(\"Matrice Originale per Inversione:\\n\", matrix_to_invert.numpy())\n",
        "print(\"Forma della Matrice Originale:\", matrix_to_invert.shape)\n",
        "\n",
        "try:\n",
        "    # Calcola l'inversa della matrice\n",
        "    matrix_inverse = tf.linalg.inv(matrix_to_invert)\n",
        "    print(\"\\nMatrice Inversa:\\n\", matrix_inverse.numpy())\n",
        "    print(\"Forma della Matrice Inversa:\", matrix_inverse.shape)\n",
        "\n",
        "    # Verifica moltiplicando la matrice originale per la sua inversa\n",
        "    identity_check = tf.matmul(matrix_to_invert, matrix_inverse)\n",
        "    print(\"\\nVerifica (Matrice Originale @ Matrice Inversa):\\n\", identity_check.numpy())\n",
        "    # Le differenze minime sono dovute a imprecisioni in virgola mobile, quindi verifichiamo la vicinanza all'identità\n",
        "    is_identity = tf.reduce_all(tf.abs(identity_check - tf.eye(matrix_to_invert.shape[0])) < 1e-4)\n",
        "    print(f\"Il prodotto è una matrice identità (entro una piccola tolleranza): {is_identity.numpy()}\")\n",
        "\n",
        "except tf.errors.InvalidArgumentError as e:\n",
        "    print(f\"\\nErrore durante l'inversione della matrice: {e}\")\n",
        "    print(\"La matrice potrebbe non essere invertibile (es. determinante nullo).\")\n",
        "\n",
        "# Esempio di matrice non invertibile per dimostrare l'errore\n",
        "print(\"\\n--- Esempio con Matrice Non Invertibile ---\")\n",
        "non_invertible_matrix = tf.constant([[1.0, 2.0],\n",
        "                                     [2.0, 4.0]], dtype=tf.float32) # Determinante = 1*4 - 2*2 = 0\n",
        "print(\"Matrice Non Invertibile:\\n\", non_invertible_matrix.numpy())\n",
        "\n",
        "try:\n",
        "    tf.linalg.inv(non_invertible_matrix)\n",
        "except tf.errors.InvalidArgumentError as e:\n",
        "    print(f\"Errore atteso per matrice non invertibile: {e}\")\n"
      ],
      "id": "12c8fce6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc0fe88e"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Rivedi e finalizza le celle aggiunte, assicurandoti che siano ben commentate e pertinenti al contesto di un corso di deep learning.\n"
      ],
      "id": "bc0fe88e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98c03149"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "The analysis involved demonstrating fundamental and advanced tensor operations in both PyTorch and TensorFlow, crucial for deep learning model development.\n",
        "\n",
        "*   **PyTorch Tensor Operations:**\n",
        "    *   **Matrix Multiplication and Element-wise Operations:** Basic matrix multiplication (`torch.matmul`) of (2,3) and (3,4) tensors resulted in a (2,4) tensor. Element-wise multiplication of (2,3) tensors produced a (2,3) tensor.\n",
        "    *   **Broadcasting:** Successfully applied scalar broadcasting (e.g., (2,3) tensor multiplied by a scalar resulted in a (2,3) tensor) and tensor broadcasting (e.g., (2,3) tensor multiplied by a (1,3) tensor resulted in a (2,3) tensor).\n",
        "    *   **Reshaping and Transposition:** Tensors were reshaped from (2,3) to (3,2) and (6,). Transposition using both `.T` and `torch.transpose` correctly changed a (2,3) tensor to a (3,2) tensor.\n",
        "    *   **Forward Propagation:** A simple forward pass through a `nn.Linear` layer (10 input features, 5 output features) followed by a `nn.ReLU` activation transformed an input tensor from shape (1,10) to (1,5).\n",
        "    *   **Backward Propagation (Autograd):** Demonstrated automatic differentiation for `y = x^2` where $x = [1, 2, 3]$. The computed gradients $x.\\text{grad}$ were $[2, 4, 6]$, matching the analytical derivative $2x$.\n",
        "    *   **Concatenation and Stacking:** `torch.cat` demonstrated concatenation along `dim=0` (two (2,3) tensors yielded a (4,3) tensor) and `dim=1` (a (2,2) and a (2,3) tensor yielded a (2,5) tensor). `torch.stack` (two (2,3) tensors) created a new dimension, resulting in a (2,2,3) tensor when stacked along `dim=0` or `dim=1`.\n",
        "    *   **Reduction Operations:** `torch.sum`, `torch.mean`, and `torch.max` were applied globally and along specific dimensions for a 3x4 tensor. For example, summing along `dim=0` resulted in a (4,) shaped tensor, and along `dim=1` resulted in a (3,) shaped tensor.\n",
        "\n",
        "*   **TensorFlow Tensor Operations:**\n",
        "    *   **Complex Broadcasting:** Illustrated advanced broadcasting rules, where a (4,1,3) tensor added to a (1,5,3) tensor resulted in a (4,5,3) tensor. Similarly, a (5,1) tensor multiplied by a (1,3) tensor yielded a (5,3) tensor.\n",
        "    *   **Forward Propagation (`tf.keras.layers.Dense`):** A forward pass through a `tf.keras.layers.Dense` layer (10 input features, 5 output features) with a `tf.keras.layers.ReLU` activation transformed an input tensor from shape (1,10) to (1,5), similar to PyTorch's linear layer.\n",
        "    *   **Automatic Differentiation (`tf.GradientTape`):** Demonstrated gradient computation for `y = x^2` with $x=3.0$ using `tf.GradientTape`, correctly yielding a gradient of $6.0$.\n",
        "    *   **Complex Matrix Operations:** Successfully performed Hadamard product (element-wise multiplication) on two 3x3 tensors. Matrix inversion using `tf.linalg.inv` was demonstrated and verified, including robust error handling for non-invertible matrices.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The demonstrations effectively cover core tensor manipulations, linear algebra operations, and the foundations of automatic differentiation in both PyTorch and TensorFlow, providing a solid base for understanding deep learning mechanics.\n",
        "*   Future steps could involve exploring more advanced tensor operations (e.g., SVD, FFT), implementing basic neural network architectures (e.g., CNN, RNN layers), or demonstrating distributed training concepts within these frameworks.\n"
      ],
      "id": "98c03149"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}